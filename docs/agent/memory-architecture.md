# Memory Architecture

## Objectif

Permettre au pet de se souvenir des informations importantes sur les utilisateurs et les conversations, sans tout stocker.

## Principes

### 1. Recherche agentique (le LLM cherche lui-mÃªme)

Le LLM dispose de **tools** pour chercher dans sa mÃ©moire :
- `search_memories(query)` : recherche sÃ©mantique (10 rÃ©sultats max)
- `get_recent_memories(limit)` : derniÃ¨res mÃ©moires stockÃ©es

Il peut faire plusieurs recherches par conversation (`maxTurns: 5`).

### 2. Stockage sÃ©lectif

Le pet ne stocke que les **faits importants** :
- Infos sur les utilisateurs (prÃ©fÃ©rences, Ã©vÃ©nements de vie)
- DÃ©cisions, conclusions
- PAS les bavardages, salutations, etc.

### 3. DÃ©duplication intelligente

Quand le pet veut stocker :
1. Recherche si info similaire existe (score > 0.85)
2. Si oui â†’ UPDATE (delete + insert avec plus de contexte)
3. Si non â†’ INSERT nouveau

Exemple :
```
Stocke: "Mickael s'est cassÃ© l'Ã©paule"
Plus tard: "Mickael s'est cassÃ© l'Ã©paule le 10 janvier 2026"
â†’ Recherche trouve l'ancien (score 0.92)
â†’ DELETE ancien + INSERT nouveau (plus prÃ©cis)
```

## Structure des mÃ©moires

### Option retenue : Hybride (tags + vecteurs)

```typescript
interface Memory {
  id: string;
  content: string;           // Le fait en texte
  subjects: string[];        // Tags plats : ["mickael", "blessure"]
  timestamp: string;
  expiresAt: string | null;  // ISO date ou null si permanent
  source: "conversation" | "chat" | "note";
  vector: number[];          // Pour recherche sÃ©mantique
}
```

**Pourquoi pas hiÃ©rarchique (`mickael:ski:blessure`) ?**
- Chevauchements difficiles ("blessure au ski" â†’ `mickael:ski:blessure` ou `mickael:blessure:ski` ?)
- Force une taxonomie rigide
- Les tags plats + recherche vectorielle gÃ¨rent le flou naturellement

### Recherche

- **Par sÃ©mantique** : vecteur (cas gÃ©nÃ©ral)
- **Par sujet** : filtre sur `subjects` (ex: "tout sur mickael")

## Flow complet (boucle agentique)

```
User (dev): "hello !"
       â†“
[Agent] ğŸš€ Starting query
[Agent] ğŸ‘¤ From: dev {message: "hello !", time: "sam. 17 janv. 2026, 22:19"}
       â†“
LLM dÃ©cide: tool_call search_memories("dev")
       â†“
[Memory] ğŸ” Searching facts: "dev"
[Memory] Found 5 facts: [{score: 0.38, "dev = Mickael"}, ...]
       â†“
LLM voit les rÃ©sultats, dÃ©cide de rÃ©pondre:
tool_call respond({expression: "happy", message: "Coucou Mickael !"})
       â†“
[Agent] âœ… Query completed {turns: 2}
```

### Exemple avec plusieurs recherches

```
User (dev): "on a parlÃ© de quoi ?"
       â†“
LLM: tool_call get_recent_memories(10)
       â†“
Result: ["dev = Mickael", "vacances GrÃ¨ce", ...]
       â†“
LLM: tool_call search_memories("Mickael vacances")  â† Il creuse !
       â†“
Result: ["Mickael part en GrÃ¨ce en fÃ©vrier", ...]
       â†“
LLM: tool_call respond("On a parlÃ© de tes vacances en GrÃ¨ce !")
```

### Stockage d'une nouvelle info

```
User: "je me suis cassÃ© l'Ã©paule"
       â†“
LLM: tool_call respond({
  message: "Ah mince ! C'Ã©tait quand ?",
  memories: [{content: "Mickael s'est cassÃ© l'Ã©paule", subjects: ["mickael", "blessure"], ttl: null}]
})
       â†“
[Agent] ğŸ’¾ Storing memory...
[Memory] Recherche similaire â†’ rien de proche â†’ INSERT
```

## DÃ©cisions prises

### Seuils de similaritÃ©

- **Recherche : pas de seuil** â€” on retourne les 10 meilleurs rÃ©sultats triÃ©s par score, le LLM dÃ©cide ce qui est pertinent (~200 tokens max)
- **DÃ©duplication : 0.85** â€” pour dÃ©tecter si une info similaire existe dÃ©jÃ  (et la mettre Ã  jour)

Le seuil de dÃ©duplication est Ã©levÃ© pour Ã©viter d'Ã©craser des faits diffÃ©rents sur la mÃªme personne (ex: "habite Ã  Paris" vs "a un fils").

**Pourquoi pas de seuil pour la recherche ?** Un seuil de 0.5 filtrait des infos utiles comme "dev = Mickael" (score 0.38). Avec 10 rÃ©sultats max triÃ©s par score, le coÃ»t en tokens est acceptable et le LLM peut juger lui-mÃªme.

### CritÃ¨re de stockage : les connexions, pas les entitÃ©s

Le LLM connaÃ®t dÃ©jÃ  les faits gÃ©nÃ©raux (Paris existe, le ski est un sport). Ce qu'il ne connaÃ®t pas, c'est **moi**, mes proches, mes relations avec le monde.

**Ã€ stocker** : les connexions entre entitÃ©s connues
- "David est mon frÃ¨re" âœ“
- "David habite Ã  Ordizan" âœ“
- "Ordizan est un village des PyrÃ©nÃ©es" âœ— (le LLM sait dÃ©jÃ )

**Le test** : est-ce que cette info est spÃ©cifique Ã  l'utilisateur ou son entourage ?

### DensitÃ© de mÃ©moire

- Peu de mÃ©moires sur un sujet â†’ plus de choses sont importantes
- Beaucoup de mÃ©moires â†’ il faut que Ã§a apporte vraiment quelque chose de nouveau

Mais attention : "David a changÃ© de travail" reste important mÃªme avec 50 mÃ©moires sur David. C'est les **variations mineures** de ce qu'on sait dÃ©jÃ  qu'on Ã©vite.

### Pas de `forget()`

Inutile. Si une info est fausse, la correction arrive naturellement via le mÃ©canisme d'update par similaritÃ© :
- StockÃ© : "David habite Ã  Ordizan"
- User : "Non en fait David a dÃ©mÃ©nagÃ© Ã  Toulouse"
- â†’ Recherche trouve l'ancien (score Ã©levÃ©) â†’ UPDATE

### Expiration : TTL dÃ©cidÃ© par le LLM

Le LLM dÃ©cide du TTL Ã  la crÃ©ation :
- Fait durable â†’ `ttl: null`
- Ã‰tat temporaire â†’ `ttl: "7d"` (ou "1d", "30d", etc.)

Un cron fait le mÃ©nage : `DELETE WHERE expiresAt < now()`

### Stockage explicite via la rÃ©ponse JSON

Le LLM retourne ses instructions de mÃ©moire dans sa rÃ©ponse :

```json
{
  "message": "Ah mince pour ton Ã©paule ! Repose-toi bien.",
  "memories": [
    {
      "content": "Mickael s'est cassÃ© l'Ã©paule le 10 janvier 2026",
      "subjects": ["mickael", "blessure"],
      "ttl": null
    }
  ]
}
```

Ã‰tat temporaire :
```json
{
  "message": "Repose-toi bien !",
  "memories": [
    {
      "content": "Mickael est malade",
      "subjects": ["mickael", "santÃ©"],
      "ttl": "7d"
    }
  ]
}
```

**Pas d'action "update"** : le LLM dit juste "retiens Ã§a", l'agent gÃ¨re l'update via le mÃ©canisme de similaritÃ© automatiquement.

### Format TTL

DurÃ©es lisibles : `"1d"`, `"7d"`, `"30d"`, `"1h"`, etc.

L'agent parse et calcule `expiresAt` en ISO date. Simple pour le LLM Ã  gÃ©nÃ©rer.

### Pas de limite de mÃ©moires

Pas nÃ©cessaire au dÃ©but. Si Qdrant rame un jour, on ajoutera. Avec un bon TTL sur les Ã©tats temporaires, Ã§a reste gÃ©rable.

## ImplÃ©mentation

### DÃ©jÃ  fait

- [x] Qdrant (Docker local + prod)
- [x] Embeddings (OpenAI text-embedding-3-small) â†’ `server/src/memory/embedding.service.ts`
- [x] Memory service base â†’ `server/src/memory/qdrant.service.ts`
  - `indexMemory()`, `searchMemory()`, `deleteMemory()`, `listMemories()`
  - `storeFactMemory()`, `searchFacts()`, `deleteExpiredMemories()`
- [x] Live collection (contexte rÃ©cent) â†’ `server/src/memory/live.service.ts`
- [x] Digest service (extraction de faits) â†’ `server/src/memory/digest.service.ts`
- [x] Config agent â†’ `server/src/config/agent.ts` + `server/agent-config.json`

### Ã€ faire

- [ ] Cron cleanup des mÃ©moires expirÃ©es (fonction existe, pas encore schedulÃ©)

## Architecture technique

### Worker (`server/src/agent/worker.mjs`)

Le worker est un process Node.js isolÃ© qui :
- Communique avec le service via stdin/stdout (JSON)
- Contient les services mÃ©moire embarquÃ©s (fetch Qdrant/OpenAI)
- GÃ¨re les sessions par utilisateur (Map userId â†’ sessionId)
- SÃ©rialise les requÃªtes via une queue (Ã©vite les race conditions)

**Tools disponibles** :
- `search_memories(query)` : recherche sÃ©mantique, 10 rÃ©sultats max
- `get_recent_memories(limit)` : derniÃ¨res mÃ©moires (1-20)
- `respond(expression, message, memories?)` : rÃ©pondre + stocker

**Sessions** :
- Une session Claude par utilisateur (conserve le contexte de conversation)
- Timeout 15 minutes d'inactivitÃ©
- Nettoyage automatique via setInterval

### Service (`server/src/agent/service.ts`)

- Spawn et manage le worker
- Forward les requÃªtes au worker
- GÃ¨re les logs du worker â†’ console.log â†’ LogPanel
- Stocke les mÃ©moires retournÃ©es par le LLM

---

## Observation passive (Ã  venir)

Le pet peut observer passivement les conversations (salons, notes) pour enrichir sa mÃ©moire, sans qu'on lui parle directement.

### ProblÃ¨me : latence vs qualitÃ©

Un digest journalier filtre bien le bruit mais crÃ©e une latence inacceptable :
- David dit "je pars en GrÃ¨ce demain" Ã  10h dans le Lobby
- Ã€ 11h, David parle au pet â†’ le pet ne sait pas encore

### Solution : deux collections Qdrant

```
Message salon
     â”‚
     â–¼
Embedding + insert "live" collection
     â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€ Quand le pet rÃ©pond â”€â”€â”€â”€â”€â”€â”
     â”‚                                   â–¼
     â”‚                    search("live", query, limit=10)
     â”‚                                   â”‚
     â”‚                                   â–¼
     â”‚                         InjectÃ© dans le prompt
     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€ Toutes les ~6h â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                         â–¼
                              Digest LLM sur toute la collection
                                         â”‚
                                         â–¼
                              Faits importants â†’ "memories" collection
                                         â”‚
                                         â–¼
                              Clear "live" collection
```

### Collection "live" (contexte rÃ©cent)

Stocke les messages bruts du **Lobby uniquement** (pour commencer).

**1 message = 1 document Qdrant** avec payload pour reconstruire la timeline si besoin :

```typescript
// Qdrant point structure
{
  id: "msg-123",
  vector: number[],           // Embedding du content
  payload: {
    content: string,          // Le message brut
    author: string,           // Username
    room: string,             // "lobby" pour l'instant
    timestamp: string         // ISO date pour tri temporel
  }
}
```

**Quand le pet rÃ©pond** :
1. Recherche sÃ©mantique dans "live" avec la question de l'utilisateur
2. Top 10 par pertinence â†’ **injectÃ© automatiquement** dans le prompt
3. Les "ok", "lol" ont des embeddings gÃ©nÃ©riques â†’ score faible â†’ filtrÃ©s naturellement

**Format d'injection dans le prompt** :

```
[Contexte live - extraits pertinents du Lobby, pas une conversation complÃ¨te]
â€¢ david (17/01 10:23) : je pars en GrÃ¨ce demain
â€¢ david (17/01 10:48) : une semaine

[MÃ©moire - faits que tu connais]
â€¢ dev = Mickael
â€¢ David est le frÃ¨re de Mickael
â€¢ ...
```

Le prompt doit Ãªtre explicite : le contexte live n'est **pas** une conversation temporelle, juste les messages les plus pertinents par rapport Ã  la question.

### Collection "memories" (faits durables)

La collection existante. Les faits importants extraits par le digest y sont stockÃ©s avec le mÃ©canisme habituel (dÃ©duplication par similaritÃ©, TTL, etc.).

### Digest pÃ©riodique

Toutes les ~6h :
1. RÃ©cupÃ¨re tous les messages de la collection "live"
2. Le LLM extrait les **faits durables** (pas les bavardages)
3. Insert dans "memories" (avec dÃ©duplication)
4. Clear la collection "live"

### Avantages vs buffer RAM

| Aspect | Buffer RAM | Qdrant "live" |
|--------|-----------|---------------|
| Filtre | Chronologique | Par pertinence sÃ©mantique |
| Bruit ("ok", "lol") | Inclus | Score faible â†’ filtrÃ© |
| Persistance | Perdu si crash | PersistÃ© |
| Infra | Nouveau systÃ¨me | RÃ©utilise Qdrant existant |

### DÃ©cisions observers

| Question | DÃ©cision |
|----------|----------|
| Salons Ã  observer | Lobby uniquement (pour commencer) |
| GranularitÃ© | 1 message = 1 document Qdrant |
| Metadata | `author`, `room`, `timestamp` dans le payload |
| Injection | Automatique (pas de tool), le prompt distingue contexte live vs mÃ©moire |

### ImplÃ©mentation

- [x] Collection Qdrant "organizer_live" â†’ `server/src/memory/live.service.ts`
- [x] Observer : Ã©coute les messages du Lobby â†’ embedding â†’ insert "live" â†’ `server/src/utils/socketEmit.ts`
- [x] Injection auto : search "live" + format dans le prompt du pet â†’ `server/src/agent/worker.mjs`
- [x] Cron digest (~6h) : LLM extrait facts â†’ "memories" â†’ clear "live" â†’ `server/src/memory/digest.service.ts`
- [x] Endpoint admin pour forcer un digest manuel â†’ `POST /admin/digest`
- [x] Bouton Digest dans PetDebugScreen

---

## Historique des dÃ©cisions

| Date | DÃ©cision | Raison |
|------|----------|--------|
| 2026-01-17 | Tags plats vs hiÃ©rarchie | Plus flexible, gÃ¨re les chevauchements |
| 2026-01-17 | Update par similaritÃ© | Ã‰vite les doublons sans gÃ©rer des IDs manuellement |
| 2026-01-17 | Stocker les connexions | Le LLM connaÃ®t les entitÃ©s, pas les relations personnelles |
| 2026-01-17 | Pas de forget() | Les corrections passent par l'update naturel |
| 2026-01-17 | Expiration selon type | Faits durables vs Ã©tats temporaires |
| 2026-01-17 | TTL dans la rÃ©ponse JSON | Le LLM dÃ©cide de la durÃ©e, l'agent exÃ©cute |
| 2026-01-17 | Pas d'action "update" | La similaritÃ© gÃ¨re l'update automatiquement |
| 2026-01-17 | TTL lisible ("7d") | Simple pour le LLM, l'agent calcule expiresAt |
| 2026-01-17 | Pas de limite mÃ©moires | Qdrant gÃ¨re, on ajustera si besoin |
| 2026-01-17 | Boucle agentique | Le LLM cherche lui-mÃªme avec des tools, peut creuser |
| 2026-01-17 | Pas de seuil recherche | Top 10 triÃ©s par score, le LLM juge la pertinence |
| 2026-01-17 | Services mÃ©moire dans worker | Ã‰vite IPC complexe, juste des fetch |
| 2026-01-17 | Sessions par utilisateur | Chaque user a son contexte de conversation |
| 2026-01-17 | Deux collections Qdrant (live + memories) | Contexte rÃ©cent sans latence + filtrage par pertinence |
| 2026-01-18 | Vidage du live aprÃ¨s digest : on garde | Infos temporaires perdues pas graves, le live est Ã©phÃ©mÃ¨re |
| 2026-01-18 | Doublons live/mÃ©moire acceptÃ©s | Temporaires (jusqu'au prochain digest), le LLM gÃ¨re |

---

## Discussion : Vidage du live aprÃ¨s digest (2026-01-18)

### ProblÃ¨me soulevÃ©

AprÃ¨s un digest, la collection live est vidÃ©e. Les infos temporaires (ex: "j'ai mal au dos") non extraites comme faits sont perdues.

### DÃ©cision

**Garder le comportement actuel** (vider le live aprÃ¨s digest).

**Raisons** :
- Le digest tourne toutes les 6h. Perdre un Ã©tat temporaire aprÃ¨s 6h est acceptable.
- Si c'Ã©tait important, le digest devrait l'extraire avec TTL.
- Le live est Ã©phÃ©mÃ¨re, pas un historique.
- Ã‰vite les doublons live/mÃ©moire permanents.

**AmÃ©lioration future possible** : affiner le prompt du digest pour mieux extraire les Ã©tats temporaires (blessure, maladie) avec TTL.

---

## Discussion : Analyse des URLs (2026-01-18)

### Ã‰tat actuel

Les URLs dans les messages sont indexÃ©es comme texte brut. Le pet voit "Mickael a envoyÃ© https://..." mais ne connaÃ®t pas le contenu du lien.

### IdÃ©e Ã©cartÃ©e : service de fetch/rÃ©sumÃ©

Un service qui :
1. DÃ©tecte les URLs dans les messages
2. Fetch le contenu web
3. RÃ©sume avec LLM
4. Extrait des faits â†’ "Mickael a partagÃ© un article sur X"

### DÃ©cision

**On garde simple.** Le fetch automatique ajouterait :
- Latence Ã  l'indexation
- CoÃ»t LLM pour chaque lien
- Gestion des erreurs (liens morts, paywall, timeout)
- Risque de contenu inappropriÃ©

Le flow actuel suffit : le pet voit l'URL, peut demander "c'Ã©tait quoi ?", et stocke la rÃ©ponse de l'utilisateur.
