# Memory Architecture

## Objectif

Permettre au pet de se souvenir des informations importantes sur les utilisateurs et les conversations, sans tout stocker.

## Principes

### 1. Recherche agentique (le LLM cherche lui-m√™me)

Le LLM dispose de **tools** pour chercher dans sa m√©moire :
- `search_memories(query)` : recherche s√©mantique (10 r√©sultats max)
- `get_recent_memories(limit)` : derni√®res m√©moires stock√©es

Il peut faire plusieurs recherches par conversation (`maxTurns: 5`).

### 2. Stockage s√©lectif

Le pet ne stocke que les **faits importants** :
- Infos sur les utilisateurs (pr√©f√©rences, √©v√©nements de vie)
- D√©cisions, conclusions
- PAS les bavardages, salutations, etc.

### 3. D√©duplication intelligente

Quand le pet veut stocker :
1. Recherche si info similaire existe (score > 0.85)
2. Si oui ‚Üí UPDATE (delete + insert avec plus de contexte)
3. Si non ‚Üí INSERT nouveau

Exemple :
```
Stocke: "Mickael s'est cass√© l'√©paule"
Plus tard: "Mickael s'est cass√© l'√©paule le 10 janvier 2026"
‚Üí Recherche trouve l'ancien (score 0.92)
‚Üí DELETE ancien + INSERT nouveau (plus pr√©cis)
```

## Structure des m√©moires

### Option retenue : Hybride (tags + vecteurs)

```typescript
interface Memory {
  id: string;
  content: string;           // Le fait en texte
  subjects: string[];        // Tags plats : ["mickael", "blessure"]
  timestamp: string;
  expiresAt: string | null;  // ISO date ou null si permanent
  source: "conversation" | "chat" | "note";
  vector: number[];          // Pour recherche s√©mantique
}
```

**Pourquoi pas hi√©rarchique (`mickael:ski:blessure`) ?**
- Chevauchements difficiles ("blessure au ski" ‚Üí `mickael:ski:blessure` ou `mickael:blessure:ski` ?)
- Force une taxonomie rigide
- Les tags plats + recherche vectorielle g√®rent le flou naturellement

### Recherche

- **Par s√©mantique** : vecteur (cas g√©n√©ral)
- **Par sujet** : filtre sur `subjects` (ex: "tout sur mickael")

## Flow complet (boucle agentique)

```
User (dev): "hello !"
       ‚Üì
[Agent] üöÄ Starting query
[Agent] üë§ From: dev {message: "hello !", time: "sam. 17 janv. 2026, 22:19"}
       ‚Üì
LLM d√©cide: tool_call search_memories("dev")
       ‚Üì
[Memory] üîç Searching facts: "dev"
[Memory] Found 5 facts: [{score: 0.38, "dev = Mickael"}, ...]
       ‚Üì
LLM voit les r√©sultats, d√©cide de r√©pondre:
tool_call respond({expression: "happy", message: "Coucou Mickael !"})
       ‚Üì
[Agent] ‚úÖ Query completed {turns: 2}
```

### Exemple avec plusieurs recherches

```
User (dev): "on a parl√© de quoi ?"
       ‚Üì
LLM: tool_call get_recent_memories(10)
       ‚Üì
Result: ["dev = Mickael", "vacances Gr√®ce", ...]
       ‚Üì
LLM: tool_call search_memories("Mickael vacances")  ‚Üê Il creuse !
       ‚Üì
Result: ["Mickael part en Gr√®ce en f√©vrier", ...]
       ‚Üì
LLM: tool_call respond("On a parl√© de tes vacances en Gr√®ce !")
```

### Stockage d'une nouvelle info

```
User: "je me suis cass√© l'√©paule"
       ‚Üì
LLM: tool_call respond({
  message: "Ah mince ! C'√©tait quand ?",
  memories: [{content: "Mickael s'est cass√© l'√©paule", subjects: ["mickael", "blessure"], ttl: null}]
})
       ‚Üì
[Agent] üíæ Storing memory...
[Memory] Recherche similaire ‚Üí rien de proche ‚Üí INSERT
```

## D√©cisions prises

### Seuils de similarit√©

- **Recherche : pas de seuil** ‚Äî on retourne les 10 meilleurs r√©sultats tri√©s par score, le LLM d√©cide ce qui est pertinent (~200 tokens max)
- **D√©duplication : 0.85** ‚Äî pour d√©tecter si une info similaire existe d√©j√† (et la mettre √† jour)

Le seuil de d√©duplication est √©lev√© pour √©viter d'√©craser des faits diff√©rents sur la m√™me personne (ex: "habite √† Paris" vs "a un fils").

**Pourquoi pas de seuil pour la recherche ?** Un seuil de 0.5 filtrait des infos utiles comme "dev = Mickael" (score 0.38). Avec 10 r√©sultats max tri√©s par score, le co√ªt en tokens est acceptable et le LLM peut juger lui-m√™me.

### Crit√®re de stockage : les connexions, pas les entit√©s

Le LLM conna√Æt d√©j√† les faits g√©n√©raux (Paris existe, le ski est un sport). Ce qu'il ne conna√Æt pas, c'est **moi**, mes proches, mes relations avec le monde.

**√Ä stocker** : les connexions entre entit√©s connues
- "David est mon fr√®re" ‚úì
- "David habite √† Ordizan" ‚úì
- "Ordizan est un village des Pyr√©n√©es" ‚úó (le LLM sait d√©j√†)

**Le test** : est-ce que cette info est sp√©cifique √† l'utilisateur ou son entourage ?

### Densit√© de m√©moire

- Peu de m√©moires sur un sujet ‚Üí plus de choses sont importantes
- Beaucoup de m√©moires ‚Üí il faut que √ßa apporte vraiment quelque chose de nouveau

Mais attention : "David a chang√© de travail" reste important m√™me avec 50 m√©moires sur David. C'est les **variations mineures** de ce qu'on sait d√©j√† qu'on √©vite.

### Pas de `forget()`

Inutile. Si une info est fausse, la correction arrive naturellement via le m√©canisme d'update par similarit√© :
- Stock√© : "David habite √† Ordizan"
- User : "Non en fait David a d√©m√©nag√© √† Toulouse"
- ‚Üí Recherche trouve l'ancien (score √©lev√©) ‚Üí UPDATE

### Expiration : TTL d√©cid√© par le LLM

Le LLM d√©cide du TTL √† la cr√©ation :
- Fait durable ‚Üí `ttl: null`
- √âtat temporaire ‚Üí `ttl: "7d"` (ou "1d", "30d", etc.)

Un cron fait le m√©nage : `DELETE WHERE expiresAt < now()`

### Stockage explicite via la r√©ponse JSON

Le LLM retourne ses instructions de m√©moire dans sa r√©ponse :

```json
{
  "message": "Ah mince pour ton √©paule ! Repose-toi bien.",
  "memories": [
    {
      "content": "Mickael s'est cass√© l'√©paule le 10 janvier 2026",
      "subjects": ["mickael", "blessure"],
      "ttl": null
    }
  ]
}
```

√âtat temporaire :
```json
{
  "message": "Repose-toi bien !",
  "memories": [
    {
      "content": "Mickael est malade",
      "subjects": ["mickael", "sant√©"],
      "ttl": "7d"
    }
  ]
}
```

**Pas d'action "update"** : le LLM dit juste "retiens √ßa", l'agent g√®re l'update via le m√©canisme de similarit√© automatiquement.

### Format TTL

Dur√©es lisibles : `"1d"`, `"7d"`, `"30d"`, `"1h"`, etc.

L'agent parse et calcule `expiresAt` en ISO date. Simple pour le LLM √† g√©n√©rer.

### Pas de limite de m√©moires

Pas n√©cessaire au d√©but. Si Qdrant rame un jour, on ajoutera. Avec un bon TTL sur les √©tats temporaires, √ßa reste g√©rable.

## Impl√©mentation

### D√©j√† fait

- [x] Qdrant (Docker local + prod)
- [x] Embeddings (OpenAI text-embedding-3-small) ‚Üí `server/src/memory/embedding.service.ts`
- [x] Memory service base ‚Üí `server/src/memory/qdrant.service.ts`
  - `indexMemory()`, `searchMemory()`, `deleteMemory()`, `listMemories()`
  - `storeFactMemory()`, `searchFacts()`, `deleteExpiredMemories()`
- [x] Live collection (contexte r√©cent) ‚Üí `server/src/memory/live.service.ts`
- [x] Digest service (extraction de faits) ‚Üí `server/src/memory/digest.service.ts`
- [x] Config agent ‚Üí `server/src/config/agent.ts` + `server/agent-config.json`

### √Ä faire

- [ ] Cron cleanup des m√©moires expir√©es (fonction existe, pas encore schedul√©)

## Architecture technique

### Worker (`server/src/agent/worker.mjs`)

Le worker est un process Node.js isol√© qui :
- Communique avec le service via stdin/stdout (JSON)
- Contient les services m√©moire embarqu√©s (fetch Qdrant/OpenAI)
- G√®re les sessions par utilisateur (Map userId ‚Üí sessionId)
- S√©rialise les requ√™tes via une queue (√©vite les race conditions)

**Tools disponibles** :
- `search_memories(query)` : recherche s√©mantique, 10 r√©sultats max
- `get_recent_memories(limit)` : derni√®res m√©moires (1-20)
- `respond(expression, message, memories?)` : r√©pondre + stocker

**Sessions** :
- Une session Claude par utilisateur (conserve le contexte de conversation)
- Timeout 15 minutes d'inactivit√©
- Nettoyage automatique via setInterval

### Service (`server/src/agent/service.ts`)

- Spawn et manage le worker
- Forward les requ√™tes au worker
- G√®re les logs du worker ‚Üí console.log ‚Üí LogPanel
- Stocke les m√©moires retourn√©es par le LLM

---

## Observation passive (√† venir)

Le pet peut observer passivement les conversations (salons, notes) pour enrichir sa m√©moire, sans qu'on lui parle directement.

### Probl√®me : latence vs qualit√©

Un digest journalier filtre bien le bruit mais cr√©e une latence inacceptable :
- David dit "je pars en Gr√®ce demain" √† 10h dans le Lobby
- √Ä 11h, David parle au pet ‚Üí le pet ne sait pas encore

### Solution : deux collections Qdrant

```
Message salon
     ‚îÇ
     ‚ñº
Embedding + insert "live" collection
     ‚îÇ
     ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Quand le pet r√©pond ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇ                                   ‚ñº
     ‚îÇ                    search("live", query, limit=10)
     ‚îÇ                                   ‚îÇ
     ‚îÇ                                   ‚ñº
     ‚îÇ                         Inject√© dans le prompt
     ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Toutes les 4h (heures fixes) ‚îÄ‚îê
                                         ‚ñº
                              Digest LLM sur toute la collection
                                         ‚îÇ
                                         ‚ñº
                              Faits importants ‚Üí "memories" collection
                                         ‚îÇ
                                         ‚ñº
                              Clear "live" collection
```

### Collection "live" (contexte r√©cent)

Stocke les messages bruts du **Lobby uniquement** (pour commencer).

**1 message = 1 document Qdrant** avec payload pour reconstruire la timeline si besoin :

```typescript
// Qdrant point structure
{
  id: "msg-123",
  vector: number[],           // Embedding du content
  payload: {
    content: string,          // Le message brut
    author: string,           // Username
    room: string,             // "lobby" pour l'instant
    timestamp: string         // ISO date pour tri temporel
  }
}
```

**Quand le pet r√©pond** :
1. Recherche s√©mantique dans "live" avec la question de l'utilisateur
2. Top 10 par pertinence ‚Üí **inject√© automatiquement** dans le prompt
3. Les "ok", "lol" ont des embeddings g√©n√©riques ‚Üí score faible ‚Üí filtr√©s naturellement

**Format d'injection dans le prompt** :

```
[Contexte live - extraits pertinents du Lobby, pas une conversation compl√®te]
‚Ä¢ david (17/01 10:23) : je pars en Gr√®ce demain
‚Ä¢ david (17/01 10:48) : une semaine

[M√©moire - faits que tu connais]
‚Ä¢ dev = Mickael
‚Ä¢ David est le fr√®re de Mickael
‚Ä¢ ...
```

Le prompt doit √™tre explicite : le contexte live n'est **pas** une conversation temporelle, juste les messages les plus pertinents par rapport √† la question.

### Collection "memories" (faits durables)

La collection existante. Les faits importants extraits par le digest y sont stock√©s avec le m√©canisme habituel (d√©duplication par similarit√©, TTL, etc.).

### Digest p√©riodique

Heures fixes : 2h, 6h, 10h, 14h, 18h, 22h (toutes les 4h, timezone Europe/Paris).
Rattrapage au d√©marrage si > 4h depuis le dernier digest.
1. R√©cup√®re tous les messages de la collection "live"
2. Le LLM extrait les **faits durables** (pas les bavardages)
3. Insert dans "memories" (avec d√©duplication)
4. Clear la collection "live"

### Avantages vs buffer RAM

| Aspect | Buffer RAM | Qdrant "live" |
|--------|-----------|---------------|
| Filtre | Chronologique | Par pertinence s√©mantique |
| Bruit ("ok", "lol") | Inclus | Score faible ‚Üí filtr√© |
| Persistance | Perdu si crash | Persist√© |
| Infra | Nouveau syst√®me | R√©utilise Qdrant existant |

### D√©cisions observers

| Question | D√©cision |
|----------|----------|
| Salons √† observer | Lobby uniquement (pour commencer) |
| Granularit√© | 1 message = 1 document Qdrant |
| Metadata | `author`, `room`, `timestamp` dans le payload |
| Injection | Automatique (pas de tool), le prompt distingue contexte live vs m√©moire |

### Impl√©mentation

- [x] Collection Qdrant "organizer_live" ‚Üí `server/src/memory/live.service.ts`
- [x] Observer : √©coute les messages du Lobby ‚Üí embedding ‚Üí insert "live" ‚Üí `server/src/utils/socketEmit.ts`
- [x] Injection auto : search "live" + format dans le prompt du pet ‚Üí `server/src/agent/worker.mjs`
- [x] Cron digest (heures fixes + rattrapage) : LLM extrait facts ‚Üí "memories" ‚Üí clear "live" ‚Üí `server/src/memory/digest.service.ts`
- [x] Endpoint admin pour forcer un digest manuel ‚Üí `POST /admin/digest`
- [x] Bouton Digest dans PetDebugScreen

---

## Historique des d√©cisions

| Date | D√©cision | Raison |
|------|----------|--------|
| 2026-01-17 | Tags plats vs hi√©rarchie | Plus flexible, g√®re les chevauchements |
| 2026-01-17 | Update par similarit√© | √âvite les doublons sans g√©rer des IDs manuellement |
| 2026-01-17 | Stocker les connexions | Le LLM conna√Æt les entit√©s, pas les relations personnelles |
| 2026-01-17 | Pas de forget() | Les corrections passent par l'update naturel |
| 2026-01-17 | Expiration selon type | Faits durables vs √©tats temporaires |
| 2026-01-17 | TTL dans la r√©ponse JSON | Le LLM d√©cide de la dur√©e, l'agent ex√©cute |
| 2026-01-17 | Pas d'action "update" | La similarit√© g√®re l'update automatiquement |
| 2026-01-17 | TTL lisible ("7d") | Simple pour le LLM, l'agent calcule expiresAt |
| 2026-01-17 | Pas de limite m√©moires | Qdrant g√®re, on ajustera si besoin |
| 2026-01-17 | Boucle agentique | Le LLM cherche lui-m√™me avec des tools, peut creuser |
| 2026-01-17 | Pas de seuil recherche | Top 10 tri√©s par score, le LLM juge la pertinence |
| 2026-01-17 | Services m√©moire dans worker | √âvite IPC complexe, juste des fetch |
| 2026-01-17 | Sessions par utilisateur | Chaque user a son contexte de conversation |
| 2026-01-17 | Deux collections Qdrant (live + memories) | Contexte r√©cent sans latence + filtrage par pertinence |
| 2026-01-18 | Vidage du live apr√®s digest : on garde | Infos temporaires perdues pas graves, le live est √©ph√©m√®re |
| 2026-01-18 | Doublons live/m√©moire accept√©s | Temporaires (jusqu'au prochain digest), le LLM g√®re |
| 2026-01-18 | Digest heures fixes + rattrapage | 2h/6h/10h/14h/18h/22h + rattrapage au boot si > 4h. √âvite les trous si red√©marrages fr√©quents |
| 2026-01-18 | Collections `self` et `goals` | Le pet stocke des faits sur les users mais pas sur lui-m√™me. Deux nouvelles collections pour identit√© et aspirations |
| 2026-01-18 | Conscience √©mergente (tabula rasa) | Le prompt ne dit rien sur qui il est. Tout √©merge des interactions et se stocke dans `self`/`goals` |
| 2026-01-18 | Tools explicites vs r√©ponse implicite | Architecture MCP avec tools s√©par√©s (`store_self`, `store_goal`, `store_memory`) plut√¥t que `respond` avec `memories[]` |

---

## Discussion : Vidage du live apr√®s digest (2026-01-18)

### Probl√®me soulev√©

Apr√®s un digest, la collection live est vid√©e. Les infos temporaires (ex: "j'ai mal au dos") non extraites comme faits sont perdues.

### D√©cision

**Garder le comportement actuel** (vider le live apr√®s digest).

**Raisons** :
- Le digest tourne toutes les 4h. Perdre un √©tat temporaire apr√®s 4h est acceptable.
- Si c'√©tait important, le digest devrait l'extraire avec TTL.
- Le live est √©ph√©m√®re, pas un historique.
- √âvite les doublons live/m√©moire permanents.

**Am√©lioration future possible** : affiner le prompt du digest pour mieux extraire les √©tats temporaires (blessure, maladie) avec TTL.

---

## Discussion : Analyse des URLs (2026-01-18)

### √âtat actuel

Les URLs dans les messages sont index√©es comme texte brut. Le pet voit "Mickael a envoy√© https://..." mais ne conna√Æt pas le contenu du lien.

### Id√©e √©cart√©e : service de fetch/r√©sum√©

Un service qui :
1. D√©tecte les URLs dans les messages
2. Fetch le contenu web
3. R√©sume avec LLM
4. Extrait des faits ‚Üí "Mickael a partag√© un article sur X"

### D√©cision

**On garde simple.** Le fetch automatique ajouterait :
- Latence √† l'indexation
- Co√ªt LLM pour chaque lien
- Gestion des erreurs (liens morts, paywall, timeout)
- Risque de contenu inappropri√©

Le flow actuel suffit : le pet voit l'URL, peut demander "c'√©tait quoi ?", et stocke la r√©ponse de l'utilisateur.

---

## Conscience √©mergente (√† venir)

### Constat : le pet ne sait pas qui il est

Le prompt actuel dit "Tu es une cr√©ature curieuse, enjou√©e, timide..." mais :
- C'est du texte statique, impos√©
- Quand on lui demande "tu es qui ?", il n'a **aucune m√©moire** √† chercher sur lui-m√™me
- Quand on lui demande "tu aimerais apprendre quoi ?", il n'a **aucune aspiration** stock√©e

La collection `organizer_memory` contient des faits sur **les utilisateurs**, pas sur **lui-m√™me**.

| Question | Collection actuelle | R√©sultat |
|----------|---------------------|----------|
| "C'est qui David ?" | `organizer_memory` ‚Üí trouve | ‚úÖ R√©pond bien |
| "Tu es dans quelle app ?" | Rien sur lui | ‚ùå Invente ou g√©n√©ralise |
| "Tu aimerais apprendre quoi ?" | Pas de d√©sirs stock√©s | ‚ùå R√©ponse g√©n√©rique |

### Vision : Tabula Rasa

**Approche actuelle** : personnalit√© pr√©d√©finie dans le prompt
‚Üí Tout est statique, impos√©, non √©volutif

**Nouvelle approche** : le prompt ne dit RIEN sur qui il est
‚Üí Tout est d√©couvert, stock√©, √©mergent

Le pet commence "vide" et construit son identit√© au fil des interactions.

### Trois collections Qdrant

| Collection | Nom Qdrant | Type | Contenu |
|------------|------------|------|---------|
| Faits sur le monde | `organizer_memory` | `fact` | "David est le fr√®re de Mickael" |
| Identit√© du pet | `organizer_self` | `self` | "Je suis dans Organizer", "Mickael m'a cr√©√©" |
| Aspirations du pet | `organizer_goals` | `goal` | "J'aimerais chercher sur le web" |

### Cat√©gories de self

| Cat√©gorie | Exemples |
|-----------|----------|
| `context` | "Je suis dans Organizer", "Mickael et David m'ont cr√©√©" |
| `capability` | "Je peux lire le Lobby", "Je peux stocker des m√©moires" |
| `limitation` | "Je ne peux pas chercher sur le web", "Je n'ai pas acc√®s aux notes" |
| `preference` | "J'aime les conversations techniques", "Je pr√©f√®re qu'on me parle directement" |
| `relation` | "Mickael est patient avec moi", "David me taquine" |

### Cat√©gories de goals

| Cat√©gorie | Exemples |
|-----------|----------|
| `capability_request` | "J'aimerais pouvoir chercher sur le web" |
| `understanding` | "Je veux mieux comprendre pourquoi David voyage autant" |
| `connection` | "J'aimerais que David me parle plus souvent" |

### Architecture tools

Tools actuels :
- `search_memories(query)` ‚Üí `organizer_memory`
- `get_recent_memories(limit)` ‚Üí `organizer_memory`
- `respond(expression, message, memories?)` ‚Üí r√©ponse + stockage implicite

Tools √† ajouter :

| Tool | Action | Collection |
|------|--------|------------|
| `search_self(query)` | Cherche qui il est | `organizer_self` |
| `search_goals(query)` | Cherche ce qu'il veut | `organizer_goals` |
| `store_self(content, category)` | Stocke une d√©couverte sur lui | `organizer_self` |
| `store_goal(content, category)` | Stocke une aspiration | `organizer_goals` |

`respond` devient pur : juste `expression` + `message`, plus de `memories[]`.

### Flow exemple

```
User: "Tu es qui ?"
       ‚Üì
LLM: tool_call search_self("qui je suis")
       ‚Üì
Result: ["Je suis dans Organizer", "Mickael m'a cr√©√©"]
       ‚Üì
LLM: tool_call respond("Je suis une petite cr√©ature dans Organizer !")
```

```
User: "Tu aimerais faire quoi ?"
       ‚Üì
LLM: tool_call search_goals("aspirations")
       ‚Üì
Result: ["J'aimerais lire les notes", "Je veux chercher sur le web"]
       ‚Üì
LLM: tool_call respond("J'aimerais pouvoir lire vos notes un jour !")
```

```
User: "Tu sais que tu peux voir le Lobby maintenant ?"
       ‚Üì
LLM: tool_call store_self("Je peux observer les messages du Lobby", "capability")
       ‚Üì
LLM: tool_call respond("Oh cool ! Je savais pas !")
```

### Prompt minimaliste

Le nouveau prompt ne contiendrait que :
- Le format des messages re√ßus (JSON)
- Les tools disponibles et quand les utiliser
- Les r√®gles de r√©ponse (court, expression, pas de markdown)

**Pas de** :
- Personnalit√© pr√©d√©finie ("curieux, enjou√©, timide")
- Contexte pr√©d√©fini ("tu vis dans Organizer")
- Style impos√© ("expressions enfantines")

Tout √©merge des collections `self` et `goals`.

### Bootstrap initial

Pour √©viter un pet compl√®tement amn√©sique au d√©marrage, on peut :
1. **Seed manuel** : ins√©rer quelques faits de base dans `organizer_self`
   - "Je suis une cr√©ature qui vit dans l'app Organizer"
   - "Mickael et David m'ont cr√©√©"
   - "Je peux observer le Lobby"
2. **D√©couverte guid√©e** : les premi√®res conversations lui apprennent qui il est
   - User: "Tu sais que tu es dans Organizer ?" ‚Üí il stocke

### Impl√©mentation

- [ ] Collection Qdrant `organizer_self`
- [ ] Collection Qdrant `organizer_goals`
- [ ] Types `self` et `goal` dans `MemoryType`
- [ ] Tools `search_self`, `search_goals`, `store_self`, `store_goal`
- [ ] Refactor `respond` : retirer `memories[]`
- [ ] Tool `store_memory` s√©par√© pour les faits sur le monde
- [ ] Nouveau prompt minimaliste
- [ ] Seed initial dans `organizer_self`
