# Memory Architecture

## Objectif

Permettre au pet de se souvenir des informations importantes sur les utilisateurs et les conversations, sans tout stocker.

## Principes

### 1. Recherche agentique (le LLM cherche lui-mÃªme)

Le LLM dispose de **tools** pour chercher dans sa mÃ©moire :
- `search_memories(query)` : recherche sÃ©mantique (10 rÃ©sultats max)
- `get_recent_memories(limit)` : derniÃ¨res mÃ©moires stockÃ©es

Il peut faire plusieurs recherches par conversation (`maxTurns: 5`).

### 2. Stockage sÃ©lectif

Le pet ne stocke que les **faits importants** :
- Infos sur les utilisateurs (prÃ©fÃ©rences, Ã©vÃ©nements de vie)
- DÃ©cisions, conclusions
- PAS les bavardages, salutations, etc.

### 3. DÃ©duplication intelligente

Quand le pet veut stocker :
1. Recherche si info similaire existe (score > 0.85)
2. Si oui â†’ UPDATE (delete + insert avec plus de contexte)
3. Si non â†’ INSERT nouveau

Exemple :
```
Stocke: "Mickael s'est cassÃ© l'Ã©paule"
Plus tard: "Mickael s'est cassÃ© l'Ã©paule le 10 janvier 2026"
â†’ Recherche trouve l'ancien (score 0.92)
â†’ DELETE ancien + INSERT nouveau (plus prÃ©cis)
```

## Structure des mÃ©moires

### Option retenue : Hybride (tags + vecteurs)

```typescript
interface Memory {
  id: string;
  content: string;           // Le fait en texte
  subjects: string[];        // Tags plats : ["mickael", "blessure"]
  timestamp: string;
  expiresAt: string | null;  // ISO date ou null si permanent
  source: "conversation" | "chat" | "note";
  vector: number[];          // Pour recherche sÃ©mantique
}
```

**Pourquoi pas hiÃ©rarchique (`mickael:ski:blessure`) ?**
- Chevauchements difficiles ("blessure au ski" â†’ `mickael:ski:blessure` ou `mickael:blessure:ski` ?)
- Force une taxonomie rigide
- Les tags plats + recherche vectorielle gÃ¨rent le flou naturellement

### Recherche

- **Par sÃ©mantique** : vecteur (cas gÃ©nÃ©ral)
- **Par sujet** : filtre sur `subjects` (ex: "tout sur mickael")

## Flow complet (boucle agentique)

```
User (dev): "hello !"
       â†“
[Agent] ğŸš€ Starting query
[Agent] ğŸ‘¤ From: dev {message: "hello !", time: "sam. 17 janv. 2026, 22:19"}
       â†“
LLM dÃ©cide: tool_call search_memories("dev")
       â†“
[Memory] ğŸ” Searching facts: "dev"
[Memory] Found 5 facts: [{score: 0.38, "dev = Mickael"}, ...]
       â†“
LLM voit les rÃ©sultats, dÃ©cide de rÃ©pondre:
tool_call respond({expression: "happy", message: "Coucou Mickael !"})
       â†“
[Agent] âœ… Query completed {turns: 2}
```

### Exemple avec plusieurs recherches

```
User (dev): "on a parlÃ© de quoi ?"
       â†“
LLM: tool_call get_recent_memories(10)
       â†“
Result: ["dev = Mickael", "vacances GrÃ¨ce", ...]
       â†“
LLM: tool_call search_memories("Mickael vacances")  â† Il creuse !
       â†“
Result: ["Mickael part en GrÃ¨ce en fÃ©vrier", ...]
       â†“
LLM: tool_call respond("On a parlÃ© de tes vacances en GrÃ¨ce !")
```

### Stockage d'une nouvelle info

```
User: "je me suis cassÃ© l'Ã©paule"
       â†“
LLM: tool_call respond({
  message: "Ah mince ! C'Ã©tait quand ?",
  memories: [{content: "Mickael s'est cassÃ© l'Ã©paule", subjects: ["mickael", "blessure"], ttl: null}]
})
       â†“
[Agent] ğŸ’¾ Storing memory...
[Memory] Recherche similaire â†’ rien de proche â†’ INSERT
```

## DÃ©cisions prises

### Seuils de similaritÃ©

- **Recherche : pas de seuil** â€” on retourne les 10 meilleurs rÃ©sultats triÃ©s par score, le LLM dÃ©cide ce qui est pertinent (~200 tokens max)
- **DÃ©duplication : 0.85** â€” pour dÃ©tecter si une info similaire existe dÃ©jÃ  (et la mettre Ã  jour)

Le seuil de dÃ©duplication est Ã©levÃ© pour Ã©viter d'Ã©craser des faits diffÃ©rents sur la mÃªme personne (ex: "habite Ã  Paris" vs "a un fils").

**Pourquoi pas de seuil pour la recherche ?** Un seuil de 0.5 filtrait des infos utiles comme "dev = Mickael" (score 0.38). Avec 10 rÃ©sultats max triÃ©s par score, le coÃ»t en tokens est acceptable et le LLM peut juger lui-mÃªme.

### CritÃ¨re de stockage : les connexions, pas les entitÃ©s

Le LLM connaÃ®t dÃ©jÃ  les faits gÃ©nÃ©raux (Paris existe, le ski est un sport). Ce qu'il ne connaÃ®t pas, c'est **moi**, mes proches, mes relations avec le monde.

**Ã€ stocker** : les connexions entre entitÃ©s connues
- "David est mon frÃ¨re" âœ“
- "David habite Ã  Ordizan" âœ“
- "Ordizan est un village des PyrÃ©nÃ©es" âœ— (le LLM sait dÃ©jÃ )

**Le test** : est-ce que cette info est spÃ©cifique Ã  l'utilisateur ou son entourage ?

### DensitÃ© de mÃ©moire

- Peu de mÃ©moires sur un sujet â†’ plus de choses sont importantes
- Beaucoup de mÃ©moires â†’ il faut que Ã§a apporte vraiment quelque chose de nouveau

Mais attention : "David a changÃ© de travail" reste important mÃªme avec 50 mÃ©moires sur David. C'est les **variations mineures** de ce qu'on sait dÃ©jÃ  qu'on Ã©vite.

### Pas de `forget()`

Inutile. Si une info est fausse, la correction arrive naturellement via le mÃ©canisme d'update par similaritÃ© :
- StockÃ© : "David habite Ã  Ordizan"
- User : "Non en fait David a dÃ©mÃ©nagÃ© Ã  Toulouse"
- â†’ Recherche trouve l'ancien (score Ã©levÃ©) â†’ UPDATE

### Expiration : TTL dÃ©cidÃ© par le LLM

Le LLM dÃ©cide du TTL Ã  la crÃ©ation :
- Fait durable â†’ `ttl: null`
- Ã‰tat temporaire â†’ `ttl: "7d"` (ou "1d", "30d", etc.)

Un cron fait le mÃ©nage : `DELETE WHERE expiresAt < now()`

### Stockage explicite via la rÃ©ponse JSON

Le LLM retourne ses instructions de mÃ©moire dans sa rÃ©ponse :

```json
{
  "message": "Ah mince pour ton Ã©paule ! Repose-toi bien.",
  "memories": [
    {
      "content": "Mickael s'est cassÃ© l'Ã©paule le 10 janvier 2026",
      "subjects": ["mickael", "blessure"],
      "ttl": null
    }
  ]
}
```

Ã‰tat temporaire :
```json
{
  "message": "Repose-toi bien !",
  "memories": [
    {
      "content": "Mickael est malade",
      "subjects": ["mickael", "santÃ©"],
      "ttl": "7d"
    }
  ]
}
```

**Pas d'action "update"** : le LLM dit juste "retiens Ã§a", l'agent gÃ¨re l'update via le mÃ©canisme de similaritÃ© automatiquement.

### Format TTL

DurÃ©es lisibles : `"1d"`, `"7d"`, `"30d"`, `"1h"`, etc.

L'agent parse et calcule `expiresAt` en ISO date. Simple pour le LLM Ã  gÃ©nÃ©rer.

### Pas de limite de mÃ©moires

Pas nÃ©cessaire au dÃ©but. Si Qdrant rame un jour, on ajoutera. Avec un bon TTL sur les Ã©tats temporaires, Ã§a reste gÃ©rable.

## ImplÃ©mentation

### DÃ©jÃ  fait

- [x] Qdrant (Docker local + prod)
- [x] Embeddings (OpenAI text-embedding-3-small) â†’ `server/src/memory/embedding.service.ts`
- [x] Memory service base â†’ `server/src/memory/qdrant.service.ts`
  - `indexMemory()`, `searchMemory()`, `deleteMemory()`, `listMemories()`

### Ã€ faire

- [ ] Cron cleanup des mÃ©moires expirÃ©es

## Architecture technique

### Worker (`server/src/agent/worker.mjs`)

Le worker est un process Node.js isolÃ© qui :
- Communique avec le service via stdin/stdout (JSON)
- Contient les services mÃ©moire embarquÃ©s (fetch Qdrant/OpenAI)
- GÃ¨re les sessions par utilisateur (Map userId â†’ sessionId)
- SÃ©rialise les requÃªtes via une queue (Ã©vite les race conditions)

**Tools disponibles** :
- `search_memories(query)` : recherche sÃ©mantique, 10 rÃ©sultats max
- `get_recent_memories(limit)` : derniÃ¨res mÃ©moires (1-20)
- `respond(expression, message, memories?)` : rÃ©pondre + stocker

**Sessions** :
- Une session Claude par utilisateur (conserve le contexte de conversation)
- Timeout 15 minutes d'inactivitÃ©
- Nettoyage automatique via setInterval

### Service (`server/src/agent/service.ts`)

- Spawn et manage le worker
- Forward les requÃªtes au worker
- GÃ¨re les logs du worker â†’ console.log â†’ LogPanel
- Stocke les mÃ©moires retournÃ©es par le LLM

---

## Historique des dÃ©cisions

| Date | DÃ©cision | Raison |
|------|----------|--------|
| 2026-01-17 | Tags plats vs hiÃ©rarchie | Plus flexible, gÃ¨re les chevauchements |
| 2026-01-17 | Update par similaritÃ© | Ã‰vite les doublons sans gÃ©rer des IDs manuellement |
| 2026-01-17 | Stocker les connexions | Le LLM connaÃ®t les entitÃ©s, pas les relations personnelles |
| 2026-01-17 | Pas de forget() | Les corrections passent par l'update naturel |
| 2026-01-17 | Expiration selon type | Faits durables vs Ã©tats temporaires |
| 2026-01-17 | TTL dans la rÃ©ponse JSON | Le LLM dÃ©cide de la durÃ©e, l'agent exÃ©cute |
| 2026-01-17 | Pas d'action "update" | La similaritÃ© gÃ¨re l'update automatiquement |
| 2026-01-17 | TTL lisible ("7d") | Simple pour le LLM, l'agent calcule expiresAt |
| 2026-01-17 | Pas de limite mÃ©moires | Qdrant gÃ¨re, on ajustera si besoin |
| 2026-01-17 | Boucle agentique | Le LLM cherche lui-mÃªme avec des tools, peut creuser |
| 2026-01-17 | Pas de seuil recherche | Top 10 triÃ©s par score, le LLM juge la pertinence |
| 2026-01-17 | Services mÃ©moire dans worker | Ã‰vite IPC complexe, juste des fetch |
| 2026-01-17 | Sessions par utilisateur | Chaque user a son contexte de conversation |
